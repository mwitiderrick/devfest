{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0270c334-2b0a-412e-9f0a-74f3b075604f",
   "metadata": {},
   "source": [
    "# Building LLM Applications With PaLM API and LangChain\n",
    "### Retrieval Augmented Generation (RAG) Using LangChain and PaLM API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e4a27b7-997f-4a21-bd8c-8c03bffa46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15206ab4-9099-42a1-b374-d4e0962d3aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"  # @param {type:\"string\"}\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e88d59a-3038-4020-aef5-75a42757a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "# Utils\n",
    "import time\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d266bf6-fc50-46f5-9b23-79d4b525edf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derrickmwiti/mukimaenv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4458e1a4-4abe-4da7-ac66-058f41ed98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"activations.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c4b7b9-4dd4-4e8a-a1d0-68a54ed531b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a2c0b5-683b-4d11-bbc1-4f75c2364899",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4cd49e-f529-4c7e-bbaa-5f262bdc5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad1361d8-55fb-4f47-b2f1-0d91248c6c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23825ca5-feb8-42ae-8763-282f0c3a30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b39735f-d536-4037-b0b0-3b5859ede1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatVertexAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4ddd5f-dc0e-4f81-97e3-3235c0b1afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ae46f2-efb3-4b4f-b1cc-c0324e6dfcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does the book talk about?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "789e966d-0363-4de1-bfc4-e85b5c9451ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The book talks about activation functions in JAX and Flax.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = qa_stuff.run(query)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f1106-7cf1-40b6-8985-2bd2becd85af",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Using LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a7f08b7-8ece-4fc1-b523-4480f44eba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_sources_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6c75385-9491-4777-89df-9e0bee124a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_with_sources_chain({\"query\":\"What is the purpose of activation functions?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "856b2642-d2f7-4ea4-b80a-2199cfd4380b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the purpose of activation functions?',\n",
       " 'result': ' Activation functions are applied in neural networks to ensure that the network outputs the desired result. ',\n",
       " 'source_documents': [Document(page_content='\\x00\\x00\\x00 Final thoughts\\nActivation functions are applied in neural networks to ensure that the network\\noutputs the desired result. The activations functions cap the output within a\\nspecific range. For instance, when solving a binary classification problem, the\\noutcome should be a number between 0 and 1. This indicates the probability of\\nan item belonging to either of the two classes. However, in a regression problem,\\nyou want the numerical prediction of a quantity, for example, the price of an', metadata={'page': 0, 'source': 'activations.pdf'}),\n",
       "  Document(page_content='07/07/2023, 09:18 Activation functions in JAX and Flax\\nhttps://www .machinelearningnuggets.com/jax-ﬂax-activation-functions/ 2/14\\x00\\x00 ELU \\x00 Exponential linear unit activation\\n\\x00\\x00 CELU \\x00 Continuously-differentiable exponential linear unit\\n\\x00\\x00 GELU\\x00 Gaussian error linear unit activation\\n\\x00\\x00\\x00 GLU \\x00 Gated linear unit activation\\n\\x00\\x00\\x00 Soft sign\\n\\x00\\x00\\x00 Softplus\\n\\x00\\x00\\x00 Swish–Sigmoid Linear Unit( SiLU\\x00\\n\\x00\\x00\\x00 Custom activation functions in JAX and Flax\\n\\x00\\x00\\x00 Final thoughts', metadata={'page': 0, 'source': 'activations.pdf'}),\n",
       "  Document(page_content='07/07/2023, 09:18 Activation functions in JAX and Flax\\nhttps://www .machinelearningnuggets.com/jax-ﬂax-activation-functions/ 8/14\\nGLU \\x00 Gated linear unit activation\\nGLU is computed as GLU(a,b)=a⊗ σ(b). It has been applied in Gated CNNs for\\nnatural language processing. In the formula, the b gate controls what information\\nis passed to the next layer. GLU helps tackle the vanishing gradient problem.\\nx = nn.glu(x)\\nSoft sign', metadata={'page': 6, 'source': 'activations.pdf'}),\n",
       "  Document(page_content='07/07/2023, 09:18 Activation functions in JAX and Flax\\nhttps://www .machinelearningnuggets.com/jax-ﬂax-activation-functions/ 6/14\\nx = nn.log_softmax (x)\\nELU \\x00 Exponential linear unit activation\\nELU activation function helps in solving the vanishing and exploding gradients\\nproblem. Unlike ReLu, ELU allows negative numbers pushing the mean unit\\nactivations closer to zero. ELUs may lead to faster training and better\\ngeneralization in networks with more than five layers.', metadata={'page': 4, 'source': 'activations.pdf'})]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2be5156-eedb-45a2-8938-bb9e23611477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Activation functions are applied in neural networks to ensure that the network outputs the desired result. \n"
     ]
    }
   ],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f511b3cb-7013-4b2d-ae3c-3c373c99cc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='\\x00\\x00\\x00 Final thoughts\\nActivation functions are applied in neural networks to ensure that the network\\noutputs the desired result. The activations functions cap the output within a\\nspecific range. For instance, when solving a binary classification problem, the\\noutcome should be a number between 0 and 1. This indicates the probability of\\nan item belonging to either of the two classes. However, in a regression problem,\\nyou want the numerical prediction of a quantity, for example, the price of an', metadata={'page': 0, 'source': 'activations.pdf'}), Document(page_content='07/07/2023, 09:18 Activation functions in JAX and Flax\\nhttps://www .machinelearningnuggets.com/jax-ﬂax-activation-functions/ 2/14\\x00\\x00 ELU \\x00 Exponential linear unit activation\\n\\x00\\x00 CELU \\x00 Continuously-differentiable exponential linear unit\\n\\x00\\x00 GELU\\x00 Gaussian error linear unit activation\\n\\x00\\x00\\x00 GLU \\x00 Gated linear unit activation\\n\\x00\\x00\\x00 Soft sign\\n\\x00\\x00\\x00 Softplus\\n\\x00\\x00\\x00 Swish–Sigmoid Linear Unit( SiLU\\x00\\n\\x00\\x00\\x00 Custom activation functions in JAX and Flax\\n\\x00\\x00\\x00 Final thoughts', metadata={'page': 0, 'source': 'activations.pdf'}), Document(page_content='07/07/2023, 09:18 Activation functions in JAX and Flax\\nhttps://www .machinelearningnuggets.com/jax-ﬂax-activation-functions/ 8/14\\nGLU \\x00 Gated linear unit activation\\nGLU is computed as GLU(a,b)=a⊗ σ(b). It has been applied in Gated CNNs for\\nnatural language processing. In the formula, the b gate controls what information\\nis passed to the next layer. GLU helps tackle the vanishing gradient problem.\\nx = nn.glu(x)\\nSoft sign', metadata={'page': 6, 'source': 'activations.pdf'}), Document(page_content='07/07/2023, 09:18 Activation functions in JAX and Flax\\nhttps://www .machinelearningnuggets.com/jax-ﬂax-activation-functions/ 6/14\\nx = nn.log_softmax (x)\\nELU \\x00 Exponential linear unit activation\\nELU activation function helps in solving the vanishing and exploding gradients\\nproblem. Unlike ReLu, ELU allows negative numbers pushing the mean unit\\nactivations closer to zero. ELUs may lead to faster training and better\\ngeneralization in networks with more than five layers.', metadata={'page': 4, 'source': 'activations.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(response['source_documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
